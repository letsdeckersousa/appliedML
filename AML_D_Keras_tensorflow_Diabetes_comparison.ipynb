{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Keras and Tensorflow Implementation for PIMA Diabetes Indians Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook compares the implementation of Keras and Tensorflow for the Pima Diabetes Indians Dataset. The neural network configuration is the same to the both approaches in two different use cases. \n",
    "\n",
    "For all of them, we have a neural network with 3 layers with 12, 8, 1 neurons. The first and second layers use the relu function and the third uses sigmoid function. Using training_epochs = 150 and:\n",
    "\n",
    "Case 1:  \n",
    "batch_size = 10\n",
    "\n",
    "Optimizer function: AdamOptimizer\n",
    "\n",
    "Case 2:\n",
    "batch_size = 5\n",
    "\n",
    "Optimizer function: RMSPropOptimizer\n",
    "\n",
    "In the end, we have the graphic comparison of the two cases for the two libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation using Keras without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians Dataset with 10-fold cross validation via sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "\n",
    "df = pd.read_csv(filename, names=names)\n",
    "array = df.values\n",
    "X = array[:,0:8]\n",
    "y = array[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurance: 69.261791 +-(5.508766)\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n",
    "# evaluate using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(model, X, y, cv=kfold)\n",
    "print \"Accurance: %f +-(%f)\"%(results.mean()*100, results.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take about 5 minutes to complete on your workstation executed on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanKeras = results.mean()*100\n",
    "stdKeras = results.std()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version of Tensorflow\n",
    "\n",
    "reference: https://www.kaggle.com/zgo2016/pima-tensorflow?scriptVersionId=1373301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Python 3 environment comes with many helpful analytics libraries installed\n",
    "It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "For example, here's several helpful packages to load in \n",
    "\n",
    "Input data files are available in the \"../input/\" directory.\n",
    "For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation using Tensorflow without optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib import learn\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "np.random.seed(seeds[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "# all the features and the Outcome - the binary classification \n",
    "names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "df = pd.read_csv(filename, names=names)\n",
    "array = df.values\n",
    "\n",
    "# X is the feature set and Y is the respective class set \n",
    "X = array[:,0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['NotDiabetes'] = 1 - df['Outcome']\n",
    "# modify the y -> 2 columns: have diabeter and not have diabetes\n",
    "Y = df[['Outcome', 'NotDiabetes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for the placeholder in the following boxes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=0.01,\n",
    "                                           global_step=1,\n",
    "                                           decay_steps=X_train.shape[0],\n",
    "                                           decay_rate=0.95,\n",
    "                                           staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_epochs = 150\n",
    "batch_size = 10\n",
    "# the step in which the epoch cost is printed\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Parameters\n",
    "n_hidden_1 = 12\n",
    "n_hidden_2 = 8\n",
    "n_hidden_3 = 1\n",
    "# Needed for the placeholder in the next box\n",
    "# how many features: 8\n",
    "n_input = X_train.shape[1]\n",
    "# how many classes: 2\n",
    "n_classes = y_train.shape[1]\n",
    "dropout = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far we have used Variables to manage our data, but there is a more basic structure, the placeholder. \n",
    "# A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our \n",
    "# operations and build our computation graph, without needing the data. In TensorFlow terminology, we then feed \n",
    "# data into the graph through these placeholders.\n",
    "\n",
    "# TensorFlow Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\")\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NN model \n",
    "# First and second layers: relu and third layer: sigmoid\n",
    "def neural_network(x, weights, biases, dropout):\n",
    "    # Hidden layer with relu activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with relu activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers weight and bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_uniform(shape=(n_input,n_hidden_1),minval=0, maxval=0.005, dtype=tf.float32, seed=0)),\n",
    "    'h2': tf.Variable(tf.random_uniform(shape=(n_hidden_1, n_hidden_2),minval=0, maxval=0.005, dtype=tf.float32, seed=0)),\n",
    "    'h3': tf.Variable(tf.random_uniform(shape=(n_hidden_2, n_hidden_3),minval=0, maxval=0.005, dtype=tf.float32, seed=0)),\n",
    "    'out': tf.Variable(tf.random_uniform(shape=(n_hidden_3, n_classes),minval=0, maxval=0.005, dtype=tf.float32, seed=0))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_uniform([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_uniform([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_uniform([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_uniform([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing model\n",
    "pred = neural_network(x, weights, biases, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-a75c7e15ad31>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining loss and optimizer - AdamOptimizer\n",
    "cost = tf.nn.l2_loss(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch:', '1', 'cost=', '2.2897')\n",
      "('epoch:', '51', 'cost=', '2.2829')\n",
      "('epoch:', '101', 'cost=', '2.2829')\n",
      "('Accuracy:', 63.77952694892883)\n",
      "('epoch:', '1', 'cost=', '2.3322')\n",
      "('epoch:', '51', 'cost=', '2.3141')\n",
      "('epoch:', '101', 'cost=', '2.3141')\n",
      "('Accuracy:', 67.32283234596252)\n",
      "('epoch:', '1', 'cost=', '2.2760')\n",
      "('epoch:', '51', 'cost=', '2.2270')\n",
      "('epoch:', '101', 'cost=', '2.2249')\n",
      "('Accuracy:', 58.26771855354309)\n",
      "('epoch:', '1', 'cost=', '2.3361')\n",
      "('epoch:', '51', 'cost=', '2.3111')\n",
      "('epoch:', '101', 'cost=', '2.3102')\n",
      "('Accuracy:', 67.32283234596252)\n",
      "('epoch:', '1', 'cost=', '2.7944')\n",
      "('epoch:', '51', 'cost=', '2.3359')\n",
      "('epoch:', '101', 'cost=', '2.3359')\n",
      "('Accuracy:', 69.68504190444946)\n",
      "('epoch:', '1', 'cost=', '2.3332')\n",
      "('epoch:', '51', 'cost=', '2.3377')\n",
      "('epoch:', '101', 'cost=', '2.3377')\n",
      "('Accuracy:', 67.71653294563293)\n",
      "('epoch:', '1', 'cost=', '2.4818')\n",
      "('epoch:', '51', 'cost=', '2.2919')\n",
      "('epoch:', '101', 'cost=', '2.2919')\n",
      "('Accuracy:', 63.77952694892883)\n",
      "('epoch:', '1', 'cost=', '2.5444')\n",
      "('epoch:', '51', 'cost=', '2.3015')\n",
      "('epoch:', '101', 'cost=', '2.3016')\n",
      "('Accuracy:', 64.56692814826965)\n",
      "('epoch:', '1', 'cost=', '2.8436')\n",
      "('epoch:', '51', 'cost=', '2.2967')\n",
      "('epoch:', '101', 'cost=', '2.2968')\n",
      "('Accuracy:', 64.96062874794006)\n",
      "('epoch:', '1', 'cost=', '2.3047')\n",
      "('epoch:', '51', 'cost=', '2.2827')\n",
      "('epoch:', '101', 'cost=', '2.2827')\n",
      "('Accuracy:', 63.77952694892883)\n",
      "Accuracy: 65.118110 +-(3.161613)\n"
     ]
    }
   ],
   "source": [
    "accuracy_list_tf1 = []\n",
    "\n",
    "# In place of kfold, we can you use different seeds to generate the training and test sets.\n",
    "for i in range(len(seeds)):\n",
    "    # Changing the seed to create the new training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seeds[i])\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate=0.01,\n",
    "                                           global_step=1,\n",
    "                                           decay_steps=X_train.shape[0],\n",
    "                                           decay_rate=0.95,\n",
    "                                           staircase=True)\n",
    "    # Running first session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs): #150\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(X_train) / batch_size) #5\n",
    "\n",
    "            X_batches = np.array_split(X_train, total_batch)\n",
    "            Y_batches = np.array_split(y_train, total_batch)\n",
    "\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = X_batches[i], Y_batches[i]\n",
    "                # Run optimization operation (backprop) and cost operation(to get loss value)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0: #50\n",
    "                print(\"epoch:\", '%d' % (epoch + 1), \"cost=\", \"{:.4f}\".format(avg_cost))\n",
    "\n",
    "        # Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print(\"Accuracy:\", accuracy.eval({x: X_test, y: y_test, keep_prob: 1})*100)\n",
    "        accuracy_list_tf1.append(accuracy.eval({x: X_test, y: y_test, keep_prob: 1})*100)\n",
    "\n",
    "meanTF = statistics.mean(accuracy_list_tf1)\n",
    "stdTF  = statistics.stdev(accuracy_list_tf1)\n",
    "print \"Accuracy: %f +-(%f)\"%(meanTF, stdTF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Deep Learning Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for Pima Indians Dataset with grid search via sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "inits = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [50, 100, 150]\n",
    "batches = [5, 10, 20]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.755208 using {'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.692708 (0.012075) with: {'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.680990 (0.045814) with: {'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.697917 (0.015073) with: {'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.722656 (0.015947) with: {'epochs': 50, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.712240 (0.017566) with: {'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.714844 (0.005524) with: {'epochs': 50, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.697917 (0.015733) with: {'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.670573 (0.041010) with: {'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.731771 (0.010253) with: {'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.733073 (0.018136) with: {'epochs': 100, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.733073 (0.010253) with: {'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.747396 (0.027498) with: {'epochs': 100, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.710938 (0.032369) with: {'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.696615 (0.041626) with: {'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.725260 (0.009207) with: {'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.753906 (0.024910) with: {'epochs': 150, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.755208 (0.012075) with: {'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 5}\n",
      "0.736979 (0.019225) with: {'epochs': 150, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 5}\n",
      "0.660156 (0.033146) with: {'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.648438 (0.025315) with: {'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.691406 (0.011049) with: {'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.696615 (0.015733) with: {'epochs': 50, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.708333 (0.024360) with: {'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.697917 (0.014382) with: {'epochs': 50, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.666667 (0.023939) with: {'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.561198 (0.169501) with: {'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.694010 (0.030314) with: {'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.731771 (0.033197) with: {'epochs': 100, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.717448 (0.009744) with: {'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.700521 (0.010253) with: {'epochs': 100, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.694010 (0.011201) with: {'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.696615 (0.014731) with: {'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.738281 (0.024080) with: {'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.714844 (0.011500) with: {'epochs': 150, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.743490 (0.025582) with: {'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 10}\n",
      "0.746094 (0.032369) with: {'epochs': 150, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 10}\n",
      "0.597656 (0.151993) with: {'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.661458 (0.023939) with: {'epochs': 50, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.674479 (0.016367) with: {'epochs': 50, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.692708 (0.022628) with: {'epochs': 50, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.695312 (0.016877) with: {'epochs': 50, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.694010 (0.014382) with: {'epochs': 50, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.666667 (0.038051) with: {'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.680990 (0.016053) with: {'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.714844 (0.015947) with: {'epochs': 100, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.710938 (0.009568) with: {'epochs': 100, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.721354 (0.012890) with: {'epochs': 100, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.717448 (0.017566) with: {'epochs': 100, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.678385 (0.018136) with: {'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.714844 (0.022326) with: {'epochs': 150, 'init': 'glorot_uniform', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.733073 (0.031948) with: {'epochs': 150, 'init': 'normal', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.720052 (0.029635) with: {'epochs': 150, 'init': 'normal', 'optimizer': 'adam', 'batch_size': 20}\n",
      "0.718750 (0.016573) with: {'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 20}\n",
      "0.726562 (0.022097) with: {'epochs': 150, 'init': 'uniform', 'optimizer': 'adam', 'batch_size': 20}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "meanKerasOpt = 0\n",
    "stdsKerasOpt = 0\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    if meanKerasOpt < mean:\n",
    "        meanKerasOpt = mean\n",
    "        stdsKerasOpt = stdev       \n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "meanKerasOpt*=100\n",
    "stdsKerasOpt*=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take about 5 minutes to complete on your workstation executed on the CPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation using Tensorflow with optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.755208 using {'epochs': 150, 'init': 'uniform', 'optimizer': 'rmsprop', 'batch_size': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output\n",
    "\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib import learn\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "np.random.seed(seeds[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "# all the features and the Outcome - the binary classification \n",
    "names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "df = pd.read_csv(filename, names=names)\n",
    "array = df.values\n",
    "\n",
    "# X is the feature set and Y is the respective class set \n",
    "X = array[:,0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more column\n",
    "df['NotDiabetes'] = 1 - df['Outcome']\n",
    "# modify the y - 2 columns: have diabeter and not have diabetes\n",
    "Y = df[['Outcome', 'NotDiabetes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_epochs = 150\n",
    "batch_size = 5 #10\n",
    "# the step in which the epoch cost is printed\n",
    "display_step = 50 #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for the placeholder in the following boxes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n",
    "learning_rate = tf.train.exponential_decay(learning_rate=0.01,\n",
    "                                           global_step=1,\n",
    "                                           decay_steps=X_train.shape[0],\n",
    "                                           decay_rate=0.95,\n",
    "                                           staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Parameters\n",
    "n_hidden_1 = 12\n",
    "n_hidden_2 = 8\n",
    "n_hidden_3 = 1\n",
    "# Needed in the next box for the placeholder\n",
    "# how many features: 8\n",
    "n_input = X_train.shape[1]\n",
    "# how many classes: 2\n",
    "n_classes = y_train.shape[1]\n",
    "dropout = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So far we have used Variables to manage our data, but there is a more basic structure, the placeholder. \n",
    "# A placeholder is simply a variable that we will assign data to at a later date. It allows us to create our \n",
    "# operations and build our computation graph, without needing the data. In TensorFlow terminology, we then feed \n",
    "# data into the graph through these placeholders.\n",
    "\n",
    "# TensorFlow Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\")\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NN model \n",
    "# First and second layers: relu\n",
    "# third layer: sigmoid\n",
    "def neural_network(x, weights, biases, dropout):\n",
    "    # Hidden layer with relu activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with relu activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "\n",
    "    #out_layer = tf.nn.dropout(out_layer, dropout)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers weight and bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_uniform(shape=(n_input,n_hidden_1),minval=0, maxval=0.005, dtype=tf.float32, seed=0)),\n",
    "    'h2': tf.Variable(tf.random_uniform(shape=(n_hidden_1, n_hidden_2),minval=0, maxval=0.005, dtype=tf.float32, seed=0)),\n",
    "    'h3': tf.Variable(tf.random_uniform(shape=(n_hidden_2, n_hidden_3),minval=0, maxval=0.005, dtype=tf.float32, seed=0)),\n",
    "    'out': tf.Variable(tf.random_uniform(shape=(n_hidden_3, n_classes),minval=0, maxval=0.005, dtype=tf.float32, seed=0))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_uniform([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_uniform([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_uniform([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_uniform([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing model\n",
    "pred = neural_network(x, weights, biases, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining loss and optimizer\n",
    "cost = tf.nn.l2_loss(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer2 = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch:', '1', 'cost=', '1.1451')\n",
      "('epoch:', '51', 'cost=', '1.1256')\n",
      "('epoch:', '101', 'cost=', '1.1472')\n",
      "('Accuracy:', 63.77952694892883)\n",
      "('epoch:', '1', 'cost=', '1.1591')\n",
      "('epoch:', '51', 'cost=', '0.9367')\n",
      "('epoch:', '101', 'cost=', '0.9375')\n",
      "('Accuracy:', 74.40944910049438)\n",
      "('epoch:', '1', 'cost=', '1.1356')\n",
      "('epoch:', '51', 'cost=', '1.1159')\n",
      "('epoch:', '101', 'cost=', '1.1156')\n",
      "('Accuracy:', 58.26771855354309)\n",
      "('epoch:', '1', 'cost=', '1.1852')\n",
      "('epoch:', '51', 'cost=', '1.1606')\n",
      "('epoch:', '101', 'cost=', '1.1606')\n",
      "('Accuracy:', 67.32283234596252)\n",
      "('epoch:', '1', 'cost=', '1.1755')\n",
      "('epoch:', '51', 'cost=', '1.1522')\n",
      "('epoch:', '101', 'cost=', '1.0725')\n",
      "('Accuracy:', 70.8661437034607)\n",
      "('epoch:', '1', 'cost=', '1.1601')\n",
      "('epoch:', '51', 'cost=', '0.9775')\n",
      "('epoch:', '101', 'cost=', '0.9817')\n",
      "('Accuracy:', 77.95275449752808)\n",
      "('epoch:', '1', 'cost=', '1.2241')\n",
      "('epoch:', '51', 'cost=', '1.1458')\n",
      "('epoch:', '101', 'cost=', '1.1457')\n",
      "('Accuracy:', 63.77952694892883)\n",
      "('epoch:', '1', 'cost=', '1.2059')\n",
      "('epoch:', '51', 'cost=', '1.1494')\n",
      "('epoch:', '101', 'cost=', '1.1432')\n",
      "('Accuracy:', 65.74802994728088)\n",
      "('epoch:', '1', 'cost=', '1.1576')\n",
      "('epoch:', '51', 'cost=', '1.0917')\n",
      "('epoch:', '101', 'cost=', '1.0757')\n",
      "('Accuracy:', 65.74802994728088)\n",
      "('epoch:', '1', 'cost=', '1.1588')\n",
      "('epoch:', '51', 'cost=', '1.1437')\n",
      "('epoch:', '101', 'cost=', '1.1437')\n",
      "('Accuracy:', 63.77952694892883)\n",
      "Accuracy: 67.165354 +-(5.774874)\n"
     ]
    }
   ],
   "source": [
    "accuracy_list_tf2 = []\n",
    "\n",
    "# In place of kfold, we can you use different seeds to generate the training and test sets.\n",
    "for i in range(len(seeds)):\n",
    "    # Changing the seed to create the new training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seeds[i])\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate=0.01,\n",
    "                                           global_step=1,\n",
    "                                           decay_steps=X_train.shape[0],\n",
    "                                           decay_rate=0.95,\n",
    "                                           staircase=True)\n",
    "    # Running first session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(X_train) / batch_size)\n",
    "\n",
    "            X_batches = np.array_split(X_train, total_batch)\n",
    "            Y_batches = np.array_split(y_train, total_batch)\n",
    "\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = X_batches[i], Y_batches[i]\n",
    "                # Run optimization operation (backprop) and cost operation(to get loss value)\n",
    "                _, c = sess.run([optimizer2, cost], feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batch\n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print(\"epoch:\", '%d' % (epoch + 1), \"cost=\", \"{:.4f}\".format(avg_cost))\n",
    "\n",
    "        # Test model\n",
    "        correct_prediction2 = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy2 = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32))\n",
    "        print(\"Accuracy:\", accuracy2.eval({x: X_test, y: y_test, keep_prob: 1})*100)\n",
    "        accuracy_list_tf2.append(accuracy2.eval({x: X_test, y: y_test, keep_prob: 1})*100)\n",
    "\n",
    "meanTFOpt = statistics.mean(accuracy_list_tf2)\n",
    "stdTFOpt  = statistics.stdev(accuracy_list_tf2)\n",
    "print \"Accuracy: %f +-(%f)\"%(meanTFOpt, stdTFOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between Keras and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y = np.array([ meanKeras, meanKerasOpt, meanTF, meanTFOpt])\n",
    "names = np.array(['keras1','keras2','tf1', 'tf2'])\n",
    "e = np.array([stdKeras, stdsKerasOpt, stdTF, stdTFOpt])\n",
    "\n",
    "plt.errorbar(names, y, e, linestyle='None', marker='^')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the implementations are not statiscally different in all the cases. The strong conclusion is: Keras2 (optimized implementation of Keras) is better than the both implementation of tensorflow. All the rest results are initially inconclusive. It is necessary to make others tests to conclude which one is better than the others (t-test student for example)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
